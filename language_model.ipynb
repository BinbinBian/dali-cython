{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from IPython.display import clear_output\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from test_dali import LSTM, Mat, LSTMState, StackedLSTM, MatOps, Layer, Graph, AdaDelta, AdaGrad, config, random as drandom, NoBackprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "was: gpu\n",
      "is gpu\n"
     ]
    }
   ],
   "source": [
    "print(\"was:\", config.default_device)\n",
    "config.default_device = 'gpu'\n",
    "print(\"is\", config.default_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cpu_allocated': False, 'gpu_allocated': False, 'cpu_fresh': False, 'gpu_fresh': False}\n",
      "{'cpu_allocated': False, 'gpu_allocated': True, 'cpu_fresh': False, 'gpu_fresh': True}\n"
     ]
    }
   ],
   "source": [
    "x = Mat(4,4)\n",
    "print(x.memory_status())\n",
    "x.sum()\n",
    "print(x.memory_status())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = drandom.uniform(0.5, size=(3,3), dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cpu_allocated': False,\n",
       " 'cpu_fresh': False,\n",
       " 'gpu_allocated': True,\n",
       " 'gpu_fresh': True}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Throttled(object):\n",
    "    decorated_to_throttled = {}\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.last_time = None\n",
    "        \n",
    "    def maybe_run(self, min_time_since_last_run_s, f):\n",
    "        now = time.time()\n",
    "        if self.last_time is None or (now - self.last_time) > min_time_since_last_run_s:\n",
    "            self.last_time = now\n",
    "            return f()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def throttled(min_time_between_run_s):\n",
    "    def decorator(f):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            if f not in Throttled.decorated_to_throttled:\n",
    "                Throttled.decorated_to_throttled[f] = Throttled()\n",
    "            t = Throttled.decorated_to_throttled[f]\n",
    "            def ok_this_is_getting_ridiculous():\n",
    "                return f(*args, **kwargs)\n",
    "            return t.maybe_run(min_time_between_run_s, ok_this_is_getting_ridiculous)\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import types\n",
    "\n",
    "def apply_recursively_on_type(x, f, target_type, list_callback=None):\n",
    "    if type(x) == target_type:\n",
    "        return f(x)\n",
    "    elif type(x) == list or isinstance(x, types.GeneratorType):\n",
    "        ret = [ apply_recursively_on_type(el, f, target_type, list_callback) for el in x]\n",
    "        if list_callback and all(type(el) == target_type for el in x):\n",
    "            ret = list_callback(ret)\n",
    "        return ret\n",
    "    elif type(x) == dict:\n",
    "        res = {}\n",
    "        for k,v in x.items():\n",
    "            res[k] = apply_recursively_on_type(v, f, target_type, list_callback)\n",
    "        return res\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "class VocabEncoded(int):\n",
    "    pass\n",
    "\n",
    "class Vocab(object):\n",
    "    UNK = '**UNK**'\n",
    "    EOS = '**EOS**'\n",
    "    \n",
    "    def __init__(self, add_eos=True, add_unk=True):\n",
    "        self.index2word = []\n",
    "        self.word2index = {}\n",
    "        self.eos = None\n",
    "        self.unk = None\n",
    "        if add_eos:\n",
    "            self.add(Vocab.UNK)\n",
    "        if add_unk:\n",
    "            self.add(Vocab.EOS)\n",
    "            \n",
    "    def __contains__(self, key):\n",
    "        if type(key) == int:\n",
    "            return key in range(len(self.index2word))\n",
    "        elif type(key) == str:\n",
    "            return key in self.word2index\n",
    "        else:\n",
    "            raise ValueError(\"expected(index or string)\")\n",
    "\n",
    "    def add(self, obj):\n",
    "        def add_f(word):\n",
    "            idx = self.word2index.get(word)\n",
    "            if idx is None:\n",
    "                idx = len(self.index2word)\n",
    "                self.index2word.append(word)\n",
    "                self.word2index[word] = idx\n",
    "                if word is Vocab.UNK:\n",
    "                    self.unk = idx\n",
    "                if word is Vocab.EOS:\n",
    "                    self.eos = idx\n",
    "            return word\n",
    "        apply_recursively_on_type(obj, add_f, str)\n",
    "    \n",
    "    def words(self):\n",
    "        return self.word2index.keys()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.index2word)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if type(index) == int:\n",
    "            return self.index2word[index]\n",
    "        elif type(index) == str:\n",
    "            if self.unk is not None:\n",
    "                return VocabEncoded(self.word2index.get(index) or self.unk)\n",
    "            else:\n",
    "                return VocabEncoded(self.word2index[index])\n",
    "        else:\n",
    "            raise ValueError(\"expected(index or string)\")\n",
    "        \n",
    "    def decode(self, obj, strip_eos=False, decode_type=int):\n",
    "        def decode_f(word_idx):\n",
    "            return self.index2word[word_idx]\n",
    "        def decode_list_f(lst):\n",
    "            if strip_eos:\n",
    "                assert self.eos is not None\n",
    "                return [el for el in lst if el != Vocab.EOS]\n",
    "            else:\n",
    "                return lst\n",
    "        return apply_recursively_on_type(obj, decode_f, decode_type, list_callback=decode_list_f)\n",
    "                \n",
    "    def encode(self, obj, add_eos=False):\n",
    "        def encode_f(word):\n",
    "            if self.unk is not None:\n",
    "                return VocabEncoded(self.word2index.get(word) or self.unk)\n",
    "            else:\n",
    "                return VocabEncoded(self.word2index[word])\n",
    "        def encode_list_f(lst):\n",
    "            lst = [encode_f(word) for word in lst]\n",
    "            if add_eos:\n",
    "                assert self.eos is not None\n",
    "                lst.append(VocabEncoded(self.eos))\n",
    "            return lst\n",
    "        return apply_recursively_on_type(obj, lambda x:x, str, list_callback=encode_list_f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vocab_test1():\n",
    "    vocab = Vocab()\n",
    "    vocab.add([[{\n",
    "        'interesting_words': ['awesome', 'cat', 'lol'],\n",
    "        'daniel' : 'daniel',\n",
    "        'wtf':[[[[[[[[[[[['there']]]]]]]]]]]]\n",
    "    }]])\n",
    "    assert(set(vocab.words()) == set(['awesome', 'there', 'daniel', '**UNK**', 'cat', '**EOS**', 'lol']))\n",
    "    original = {1:{1:{1:[[[[[ 'awesome', 'but','staph', 'daniel' ]]]]]}}}\n",
    "    original_with_unks = {1: {1: {1: [[[[['awesome', '**UNK**', '**UNK**', 'daniel']]]]]}}}\n",
    "    encoded  = vocab.encode(original)\n",
    "    decoded  = vocab.decode(encoded, decode_type=VocabEncoded)\n",
    "    assert original_with_unks == decoded\n",
    "\n",
    "    encoded  = vocab.encode(original, add_eos=True)\n",
    "    decoded  = vocab.decode(encoded, strip_eos=True, decode_type=VocabEncoded)\n",
    "    assert original_with_unks == decoded\n",
    "vocab_test1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def process_files(files, mapper, reducer):\n",
    "    if files == str:\n",
    "        files = [files]\n",
    "    for file in files:\n",
    "        for element in mapper(file):\n",
    "            for res in reducer(element):\n",
    "                yield res\n",
    "\n",
    "def discover_files(root_path, extension=None):\n",
    "    for path, dirs, files in os.walk(root_path):\n",
    "        for file in files:\n",
    "            if extension is None or file.endswith(extension):\n",
    "                yield os.path.join(path, file)\n",
    "\n",
    "class Mapper(object):\n",
    "    FILTER      = 1\n",
    "    TRANSFORMER = 2\n",
    "    def __init__(self, map_f):\n",
    "        self.map_f      = map_f\n",
    "        self._transformations = []\n",
    "        \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        for element in self.map_f(*args, **kwargs):\n",
    "            ignore = False\n",
    "            for transform_f in self._transformations:\n",
    "                element = transform_f(element)\n",
    "                if element is None:\n",
    "                    ignore = True\n",
    "                    break\n",
    "            if ignore:\n",
    "                continue\n",
    "            yield element\n",
    "    \n",
    "    def add_filter(self, filter_f):\n",
    "        def wrapper(element):\n",
    "            if filter_f(element):\n",
    "                return element\n",
    "            return None\n",
    "        self.add_transform(wrapper)\n",
    "        return self\n",
    "    \n",
    "    def add_transform(self, transform_f):\n",
    "        self._transformations.append(transform_f)\n",
    "        return self\n",
    "            \n",
    "class LineExtractor(Mapper):\n",
    "    def __init__(self):\n",
    "        def extract_lines(file):\n",
    "            with open(file, \"rt\") as f:\n",
    "                for line in f:\n",
    "                    yield line[:-1]\n",
    "        super(LineExtractor, self).__init__(extract_lines)\n",
    "            \n",
    "    def lower(self):\n",
    "        return self.add_transform(lambda x: x.lower())\n",
    "        \n",
    "            \n",
    "    def bound_length(self, lower_bound=None, upper_bound=None):\n",
    "        if lower_bound:\n",
    "            self.add_filter(lambda x: lower_bound <= len(x))\n",
    "        if lower_bound:\n",
    "            self.add_filter(lambda x: len(x) <= upper_bound)\n",
    "        return self\n",
    "    \n",
    "    def split_spaces(self):\n",
    "        return self.add_transform(lambda x: x.split(' '))\n",
    "\n",
    "def batched_reducer(minibatch_size,\n",
    "                    minibatch_f=lambda x:x,\n",
    "                    examples_until_minibatches=None,\n",
    "                    sorting_key=lambda x: len(x)):\n",
    "    collected = []\n",
    "    examples_until_minibatches = examples_until_minibatches or minibatch_size\n",
    "    def wrapper(el):\n",
    "        collected.append(el)\n",
    "        if len(collected) >= examples_until_minibatches:\n",
    "            collected.sort(key=sorting_key)\n",
    "            batch_start_idxes = list(range(0, len(collected), minibatch_size))\n",
    "            random.shuffle(batch_start_idxes)\n",
    "            for i in batch_start_idxes:\n",
    "                if i + minibatch_size < len(collected):\n",
    "                    yield minibatch_f(collected[i:(i + minibatch_size)])\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Batch(object):\n",
    "    def __init__(self):\n",
    "        self.timesteps = 0\n",
    "        self.examples  = 0\n",
    "    def inputs(timestep):\n",
    "        return None\n",
    "    def targets(timestep):\n",
    "        return None\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Batch(timesteps=%d, examples=%d)' % (self.timesteps, self.examples)\n",
    "\n",
    "class LMBatch(object):\n",
    "    START_TOKEN = '**START**'\n",
    "    @staticmethod\n",
    "    def given_vocab(vocab, **kwargs):\n",
    "        def wrapper(sentences):\n",
    "            return LMBatch(sentences, vocab, **kwargs)\n",
    "        return wrapper\n",
    "    \n",
    "    def __init__(self, sentences, vocab, store_originals=False, add_eos=True):\n",
    "        if store_originals:\n",
    "            self.sentences = sentences\n",
    "        sentences = [vocab.encode(s, add_eos=add_eos) for s in sentences]\n",
    "\n",
    "        self.sentence_lengths = [len(s) for s in sentences]\n",
    "\n",
    "        self.timesteps = max(self.sentence_lengths)\n",
    "        self.examples  = len(sentences)\n",
    "        # we add one index to account for start of sequence token\n",
    "        self.data = np.empty((self.timesteps + 1, self.examples))\n",
    "        # data is badded by EOS\n",
    "        self.data.fill(vocab.eos)\n",
    "        self.data[0,:].fill(vocab[LMBatch.START_TOKEN])\n",
    "        for example_idx, example in enumerate(sentences):\n",
    "            self.data[1:(len(example) + 1), example_idx] = example\n",
    "        self.data = Mat(self.data, dtype=np.int32)\n",
    "        \n",
    "    def inputs(self, timestep):\n",
    "        return self.data[timestep]\n",
    "    \n",
    "    def targets(self, timestep):\n",
    "         # predictions are offset by 1 to inputs, so\n",
    "        return self.data[timestep + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_bookcorpus(path, vocab, minibatch_size, sentences_until_minibatch=None, sentence_length_bounds=(2, 20)):\n",
    "    sentences_until_minibatch = sentences_until_minibatch or 10000 * minibatch_size\n",
    "    files   = discover_files(BOOKCORPUS, \".txt\")\n",
    "    mapper  = LineExtractor() \\\n",
    "              .lower()        \\\n",
    "              .split_spaces() \\\n",
    "              .bound_length(*sentence_length_bounds)\n",
    "    reducer = batched_reducer(minibatch_size,\n",
    "                              LMBatch.given_vocab(glove_vocab, store_originals=True),\n",
    "                              sentences_until_minibatch)\n",
    "    return process_files(files=files, mapper=mapper, reducer=reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Beam = namedtuple(\"Beam\", [\"solution\", \"score\", \"state\"])\n",
    "\n",
    "def beam_search(initial_state,\n",
    "                candidate_scores,\n",
    "                make_choice,\n",
    "                beam_width=5,\n",
    "                eos_symbol = None,\n",
    "                max_sequence_length=None,\n",
    "                blacklist=[]):\n",
    "    iterations = 0\n",
    "    results = [\n",
    "        Beam([], Mat(1,1), initial_state)\n",
    "    ]\n",
    "    \n",
    "    def lazy_beam(prev_beam, candidate, new_score):\n",
    "        def generate():\n",
    "            return Beam(\n",
    "                prev_beam.solution + [candidate],\n",
    "                new_score,\n",
    "                make_choice(prev_beam.state, candidate),\n",
    "            )           \n",
    "        return generate\n",
    "    \n",
    "    def lazy_identity(beam):\n",
    "        def generate():\n",
    "            return beam\n",
    "        return generate\n",
    "    \n",
    "    while max_sequence_length is None or iterations < max_sequence_length:\n",
    "        proposals = []\n",
    "        for beam in results:\n",
    "            if (eos_symbol is not None and\n",
    "                    len(beam.solution) > 0 and\n",
    "                    beam.solution[-1] == eos_symbol):\n",
    "                proposals.append((beam.score, lazy_identity(beam)))\n",
    "            else:\n",
    "                scores = candidate_scores(beam.state)\n",
    "                sorted_candidates = MatOps.argsort(scores)\n",
    "                \n",
    "                sorted_candidates = sorted_candidates[::-1]\n",
    "                candidates_remaining = beam_width\n",
    "                for candidate_idx in sorted_candidates:\n",
    "                    if candidate_idx in blacklist:\n",
    "                        continue\n",
    "                    new_score = beam.score + scores.T()[candidate_idx]\n",
    "                    proposals.append((new_score, lazy_beam(beam, candidate_idx, new_score)))\n",
    "                    candidates_remaining -= 1\n",
    "                    if candidates_remaining <= 0:\n",
    "                        break\n",
    "        proposals.sort(reverse=True, key=lambda x: x[0].w[0])\n",
    "        results = [ eval_beam() for _, eval_beam in proposals[:beam_width]]\n",
    "        \n",
    "        iterations += 1\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "def beam_search_test():\n",
    "    MAX_LENGTH = 2\n",
    "    choices = {\n",
    "        #initial_choices\n",
    "        \"a\": 0.6,\n",
    "        \"b\": 0.4,\n",
    "        #after chosing a\n",
    "        \"aa\": 0.55,  # (total worth 0.33)\n",
    "        \"ab\": 0.45,  # (total worth 0.18)\n",
    "        #after choosing b\n",
    "        \"ba\": 0.99,  # (total worth 0.495)\n",
    "        \"bb\": 0.11,  # (total worth 0.044)\n",
    "    };\n",
    "\n",
    "    # Above example is designed to demonstrate greedy solution,\n",
    "    # as well as better optimal solution:\n",
    "    # GREEDY:    (beam_width == 1) => \"aa\" worth 0.33\n",
    "    # OPTIMAL:   (beam_width == 2) => \"ba\" worth 0.495\n",
    "    res_aa = Beam([0,0], Mat([math.log(0.6 * 0.55)]), \"aa\")\n",
    "    res_ab = Beam([0,1], Mat([math.log(0.6 * 0.45)]), \"ab\")\n",
    "    res_ba = Beam([1,0], Mat([math.log(0.4 * 0.99)]), \"ba\")\n",
    "    res_bb = Beam([1,1], Mat([math.log(0.4 * 0.11)]), \"bb\")\n",
    "\n",
    "    initial_state = \"\";\n",
    "    def candidate_scores(state):\n",
    "        ret = Mat(1,2)\n",
    "        ret.w[0,0] = math.log(choices[state + \"a\"])\n",
    "        ret.w[0,1] = math.log(choices[state + \"b\"])\n",
    "        return ret\n",
    "    def make_choice(prev_state, choice):\n",
    "        return prev_state + (\"a\" if choice == 0 else \"b\")\n",
    "    \n",
    "    def my_beam_search(beam_width):\n",
    "        return beam_search(initial_state=initial_state,\n",
    "                           candidate_scores=candidate_scores,\n",
    "                           make_choice=make_choice,\n",
    "                           beam_width=beam_width,\n",
    "                           max_sequence_length = MAX_LENGTH)\n",
    "   \n",
    "    def beams_equal(b1, b2):\n",
    "        return (b1.solution == b2.solution and \n",
    "                np.allclose(b1.score.w, b2.score.w) and \n",
    "                b1.state == b2.state)\n",
    "    \n",
    "    def results_equal(a,b):\n",
    "        return len(a) == len(b) and all(beams_equal(b1,b2) for b1,b2 in zip(a,b))\n",
    "\n",
    "    #EXPECT_THROW(my_beam_search(0),std::runtime_error);\n",
    "    assert results_equal(my_beam_search(1), [res_aa])\n",
    "    assert results_equal(my_beam_search(2), [res_ba, res_aa])\n",
    "    assert results_equal(my_beam_search(4), [res_ba, res_aa, res_ab, res_bb])\n",
    "    assert results_equal(my_beam_search(10),[res_ba, res_aa, res_ab, res_bb])\n",
    "    \n",
    "beam_search_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LanguageModel(object):\n",
    "    def __init__(self, input_size, hiddens, vocab_size, dtype=np.float32):\n",
    "        self.input_size = input_size\n",
    "        self.hiddens    = hiddens\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        self.encoder = drandom.uniform(-0.05, 0.05, (vocab_size, input_size), dtype=dtype)\n",
    "        self.lstm    = StackedLSTM(input_size, hiddens, dtype=dtype)\n",
    "        self.decoder = Layer(hiddens[-1], vocab_size, dtype=dtype)\n",
    "    \n",
    "    def error(self, batch):\n",
    "        error = Mat(1,1)\n",
    "        state = self.lstm.initial_states()\n",
    "        for ts in range(batch.timesteps):\n",
    "            inputs  = batch.inputs(ts)\n",
    "            targets = batch.targets(ts)\n",
    "            if inputs:\n",
    "                encoded = self.encoder[batch.inputs(ts)]\n",
    "            else:\n",
    "                encoded = Mat(1, self.input_size)\n",
    "            state = self.lstm.activate(encoded, state)\n",
    "            if targets:\n",
    "                decoded = self.decoder.activate(state[-1].hidden)\n",
    "                error = error + MatOps.softmax_cross_entropy(decoded, targets).sum()\n",
    "        return error\n",
    "    \n",
    "    def sample(self, priming, temperature=1.0, **kwargs):\n",
    "        #temperature = kwargs.get(\"temperature\", 1.0)\n",
    "        with NoBackprop():\n",
    "            state = self.lstm.initial_states()\n",
    "            for word_idx in priming:\n",
    "                encoded = self.encoder[word_idx]\n",
    "                state = self.lstm.activate(encoded, state)\n",
    "            def candidate_scores(state):\n",
    "                return MatOps.softmax(self.decoder.activate(state[-1].hidden), temperature=temperature).log()\n",
    "            def make_choice(state, candidate_idx):\n",
    "                encoded = self.encoder[candidate_idx]\n",
    "                return self.lstm.activate(encoded, state)\n",
    "\n",
    "            return beam_search(state,\n",
    "                               candidate_scores,\n",
    "                               make_choice,\n",
    "                               **kwargs)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.encoder] + self.lstm.parameters() + self.decoder.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_reconstructions(model, words, temperature=1.0):\n",
    "    for solution, score, _ in model.sample(glove_vocab.encode([LMBatch.START_TOKEN] + words), \n",
    "                                           eos_symbol=glove_vocab.eos,\n",
    "                                           max_sequence_length=20,\n",
    "                                           blacklist=[glove_vocab.unk],\n",
    "                                           temperature=temperature):\n",
    "        score = math.exp(score.w[0])\n",
    "        priming = ' '.join(words)\n",
    "        solution = ' '.join(glove_vocab.decode(solution, False))\n",
    "        print('%f => [%s] %s' % (score, priming, solution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GLOVE_VOCAB = '/home/sidor/projects/Dali/data/glove/vocab10k.txt'\n",
    "BOOKCORPUS  = '/home/sidor/datasets/bookcorpus/'\n",
    "\n",
    "INPUT_SIZE = 250\n",
    "HIDDENS = [250, 250]\n",
    "MINIBATCH = 1024\n",
    "SENTENCES_UNTIL_MINIBATCH = 1000 * MINIBATCH\n",
    "SENTENCE_LENGTH=(2, 10)\n",
    "\n",
    "glove_vocab = Vocab()\n",
    "glove_vocab.add(LineExtractor()(GLOVE_VOCAB))\n",
    "glove_vocab.add(LMBatch.START_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model  = LanguageModel(INPUT_SIZE, HIDDENS, len(glove_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = model.parameters()\n",
    "s = AdaGrad(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s.step_size = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_generator = extract_bookcorpus(BOOKCORPUS, glove_vocab, MINIBATCH, \n",
    "                                SENTENCES_UNTIL_MINIBATCH, sentence_length_bounds=SENTENCE_LENGTH)\n",
    "error_evolution = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:              4.35426369025\n",
      "Time per batch:     0.37502988165000306\n",
      "Words per second:   20583.858777872658\n",
      "Batches processed:  59094\n",
      "Max batch length:   11\n",
      "0.077996 => [someone will take] . **EOS**\n",
      "0.002083 => [someone will take] , . **EOS**\n",
      "0.002046 => [someone will take] the . **EOS**\n",
      "0.001554 => [someone will take] to . **EOS**\n",
      "0.000077 => [someone will take] the the . **EOS**\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-689ca75309d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mbatch_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[0merror_evolution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentence_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-c7f71671db8d>\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[0mdecoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                 \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mMatOps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "total_error, num_words = 0.0, 0\n",
    "\n",
    "reports = Throttled()\n",
    "\n",
    "total_error,    num_words   = 0.0, 0\n",
    "batch_time, num_batches = 0.0, 0\n",
    "\n",
    "@throttled(5)\n",
    "def report(example):\n",
    "    if num_batches == 0 or num_words == 0 or abs(batch_time) < 1e-6:\n",
    "        return\n",
    "    clear_output()\n",
    "    print('Error:             ', total_error / num_words)\n",
    "    print('Time per batch:    ', batch_time  / num_batches)\n",
    "    print('Words per second:  ', num_words   / batch_time )\n",
    "    print('Batches processed: ', num_batches)\n",
    "    print('Max batch length:  ', max_timesteps)\n",
    "    show_reconstructions(model, example)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "max_timesteps = 0\n",
    "\n",
    "batch_end_time, batch_start_time = None, None\n",
    "\n",
    "for batch in example_generator:\n",
    "    max_timesteps = max(max_timesteps, batch.timesteps)\n",
    "    \n",
    "    batch_start_time = time.time()\n",
    "    error = model.error(batch)\n",
    "    error_evolution.append(error.w[0,0] / sum(batch.sentence_lengths))\n",
    "\n",
    "    error.grad()\n",
    "    Graph.backward()\n",
    "    s.step(params)\n",
    "    batch_end_time = time.time()\n",
    "\n",
    "    total_error += error.w[0, 0]\n",
    "    num_words   += sum(batch.sentence_lengths)\n",
    "        \n",
    "    if batch_end_time is not None and batch_start_time is not None:\n",
    "        batch_time += batch_end_time - batch_start_time\n",
    "    num_batches    += 1\n",
    "    \n",
    "    example = batch.sentences[0]\n",
    "    example_len = random.randint(1, len(example))\n",
    "    \n",
    "    report(example[:example_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe8bfefb390>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZEw/D+GzHZkXrHExAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe8c205fcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def median_smoothing(signal, window=10):\n",
    "    res = []\n",
    "    for i in range(window, len(signal)):\n",
    "        actual_window = signal[i-window:i]\n",
    "        res.append(np.median(actual_window))\n",
    "    return res\n",
    "plt.plot(median_smoothing(error_evolution, window=len(error_evolution) // 20 )[::10] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59101"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(error_evolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.967180 => [where did] . **EOS**\n",
      "0.000000 => [where did] **EOS**\n",
      "0.000000 => [where did] '' . **EOS**\n",
      "0.000000 => [where did] i . **EOS**\n",
      "0.000000 => [where did] ? . **EOS**\n"
     ]
    }
   ],
   "source": [
    "show_reconstructions(model, \"where did\".split(' '), temperature=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "openf = False\n",
    "\n",
    "import pickle\n",
    "\n",
    "if openf:\n",
    "    with open(\"/home/sidor/tmp/lm.dali\", \"rb\") as f:\n",
    "        loaded_model = pickle.load(f)\n",
    "else:\n",
    "    with open(\"/home/sidor/tmp/lm.dali\", \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "openf = False\n",
    "\n",
    "import pickle\n",
    "\n",
    "if openf:\n",
    "    with open(\"/home/sidor/tmp/lm-solver.dali\", \"rb\") as f:\n",
    "        loaded_s = pickle.load(f)\n",
    "else:\n",
    "    with open(\"/home/sidor/tmp/lm-solver.dali\", \"wb\") as f:\n",
    "        pickle.dump(s, f)\n",
    "#batches_processed = 59094"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.__class__ = LanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n",
      "c\n"
     ]
    }
   ],
   "source": [
    "w = []\n",
    "\n",
    "def lazy_beam(c):\n",
    "    def generate():\n",
    "        print(c)\n",
    "    return generate\n",
    "\n",
    "for c in [\"a\", \"b\",\"c\"]:\n",
    "    w.append(lazy_beam(c))\n",
    "\n",
    "for ww in w:\n",
    "    ww()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n",
      "c\n",
      "c\n"
     ]
    }
   ],
   "source": [
    "w = []\n",
    "\n",
    "for c in [\"a\", \"b\",\"c\"]:\n",
    "    j = c\n",
    "    def lazy_beam():\n",
    "        print(j)\n",
    "    w.append(lazy_beam)\n",
    "\n",
    "for ww in w:\n",
    "    ww()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
