{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from IPython.display import clear_output\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from test_dali import LSTM, Mat, LSTMState, StackedLSTM, MatOps, Layer, Graph, AdaDelta, config, random as drandom, NoBackprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "was: gpu\n",
      "is gpu\n"
     ]
    }
   ],
   "source": [
    "print(\"was:\", config.default_device)\n",
    "config.default_device = 'gpu'\n",
    "print(\"is\", config.default_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gpu_fresh': False, 'cpu_allocated': False, 'gpu_allocated': False, 'cpu_fresh': False}\n",
      "{'gpu_fresh': True, 'cpu_allocated': False, 'gpu_allocated': True, 'cpu_fresh': False}\n"
     ]
    }
   ],
   "source": [
    "x = Mat(4,4)\n",
    "print(x.memory_status())\n",
    "x.sum()\n",
    "print(x.memory_status())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = drandom.uniform(0.5, size=(3,3), dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cpu_allocated': False,\n",
       " 'cpu_fresh': False,\n",
       " 'gpu_allocated': True,\n",
       " 'gpu_fresh': True}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Throttled(object):\n",
    "    decorated_to_throttled = {}\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.last_time = None\n",
    "        \n",
    "    def maybe_run(self, min_time_since_last_run_s, f):\n",
    "        now = time.time()\n",
    "        if self.last_time is None or (now - self.last_time) > min_time_since_last_run_s:\n",
    "            self.last_time = now\n",
    "            return f()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def throttled(min_time_between_run_s):\n",
    "    def decorator(f):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            if f not in Throttled.decorated_to_throttled:\n",
    "                Throttled.decorated_to_throttled[f] = Throttled()\n",
    "            t = Throttled.decorated_to_throttled[f]\n",
    "            def ok_this_is_getting_ridiculous():\n",
    "                return f(*args, **kwargs)\n",
    "            return t.maybe_run(min_time_between_run_s, ok_this_is_getting_ridiculous)\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import types\n",
    "\n",
    "def apply_recursively_on_type(x, f, target_type, list_callback=None):\n",
    "    if type(x) == target_type:\n",
    "        return f(x)\n",
    "    elif type(x) == list or isinstance(x, types.GeneratorType):\n",
    "        ret = [ apply_recursively_on_type(el, f, target_type, list_callback) for el in x]\n",
    "        if list_callback and all(type(el) == target_type for el in x):\n",
    "            ret = list_callback(ret)\n",
    "        return ret\n",
    "    elif type(x) == dict:\n",
    "        res = {}\n",
    "        for k,v in x.items():\n",
    "            res[k] = apply_recursively_on_type(v, f, target_type, list_callback)\n",
    "        return res\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "class VocabEncoded(int):\n",
    "    pass\n",
    "\n",
    "class Vocab(object):\n",
    "    UNK = '**UNK**'\n",
    "    EOS = '**EOS**'\n",
    "    \n",
    "    def __init__(self, add_eos=True, add_unk=True):\n",
    "        self.index2word = []\n",
    "        self.word2index = {}\n",
    "        self.eos = None\n",
    "        self.unk = None\n",
    "        if add_eos:\n",
    "            self.add(Vocab.UNK)\n",
    "        if add_unk:\n",
    "            self.add(Vocab.EOS)\n",
    "            \n",
    "    def __contains__(self, key):\n",
    "        if type(key) == int:\n",
    "            return key in range(len(self.index2word))\n",
    "        elif type(key) == str:\n",
    "            return key in self.word2index\n",
    "        else:\n",
    "            raise ValueError(\"expected(index or string)\")\n",
    "\n",
    "    def add(self, obj):\n",
    "        def add_f(word):\n",
    "            idx = self.word2index.get(word)\n",
    "            if idx is None:\n",
    "                idx = len(self.index2word)\n",
    "                self.index2word.append(word)\n",
    "                self.word2index[word] = idx\n",
    "                if word is Vocab.UNK:\n",
    "                    self.unk = idx\n",
    "                if word is Vocab.EOS:\n",
    "                    self.eos = idx\n",
    "            return word\n",
    "        apply_recursively_on_type(obj, add_f, str)\n",
    "    \n",
    "    def words(self):\n",
    "        return self.word2index.keys()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.index2word)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if type(index) == int:\n",
    "            return self.index2word[index]\n",
    "        elif type(index) == str:\n",
    "            if self.unk is not None:\n",
    "                return VocabEncoded(self.word2index.get(index) or self.unk)\n",
    "            else:\n",
    "                return VocabEncoded(self.word2index[index])\n",
    "        else:\n",
    "            raise ValueError(\"expected(index or string)\")\n",
    "        \n",
    "    def decode(self, obj, strip_eos=False, decode_type=int):\n",
    "        def decode_f(word_idx):\n",
    "            return self.index2word[word_idx]\n",
    "        def decode_list_f(lst):\n",
    "            if strip_eos:\n",
    "                assert self.eos is not None\n",
    "                return [el for el in lst if el != Vocab.EOS]\n",
    "            else:\n",
    "                return lst\n",
    "        return apply_recursively_on_type(obj, decode_f, decode_type, list_callback=decode_list_f)\n",
    "                \n",
    "    def encode(self, obj, add_eos=False):\n",
    "        def encode_f(word):\n",
    "            if self.unk is not None:\n",
    "                return VocabEncoded(self.word2index.get(word) or self.unk)\n",
    "            else:\n",
    "                return VocabEncoded(self.word2index[word])\n",
    "        def encode_list_f(lst):\n",
    "            lst = [encode_f(word) for word in lst]\n",
    "            if add_eos:\n",
    "                assert self.eos is not None\n",
    "                lst.append(VocabEncoded(self.eos))\n",
    "            return lst\n",
    "        return apply_recursively_on_type(obj, lambda x:x, str, list_callback=encode_list_f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vocab_test1():\n",
    "    vocab = Vocab()\n",
    "    vocab.add([[{\n",
    "        'interesting_words': ['awesome', 'cat', 'lol'],\n",
    "        'daniel' : 'daniel',\n",
    "        'wtf':[[[[[[[[[[[['there']]]]]]]]]]]]\n",
    "    }]])\n",
    "    assert(set(vocab.words()) == set(['awesome', 'there', 'daniel', '**UNK**', 'cat', '**EOS**', 'lol']))\n",
    "    original = {1:{1:{1:[[[[[ 'awesome', 'but','staph', 'daniel' ]]]]]}}}\n",
    "    original_with_unks = {1: {1: {1: [[[[['awesome', '**UNK**', '**UNK**', 'daniel']]]]]}}}\n",
    "    encoded  = vocab.encode(original)\n",
    "    decoded  = vocab.decode(encoded, decode_type=VocabEncoded)\n",
    "    assert original_with_unks == decoded\n",
    "\n",
    "    encoded  = vocab.encode(original, add_eos=True)\n",
    "    decoded  = vocab.decode(encoded, strip_eos=True, decode_type=VocabEncoded)\n",
    "    assert original_with_unks == decoded\n",
    "vocab_test1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def process_files(files, mapper, reducer):\n",
    "    if files == str:\n",
    "        files = [files]\n",
    "    for file in files:\n",
    "        for element in mapper(file):\n",
    "            for res in reducer(element):\n",
    "                yield res\n",
    "\n",
    "def discover_files(root_path, extension=None):\n",
    "    for path, dirs, files in os.walk(root_path):\n",
    "        for file in files:\n",
    "            if extension is None or file.endswith(extension):\n",
    "                yield os.path.join(path, file)\n",
    "\n",
    "class Mapper(object):\n",
    "    FILTER      = 1\n",
    "    TRANSFORMER = 2\n",
    "    def __init__(self, map_f):\n",
    "        self.map_f      = map_f\n",
    "        self._transformations = []\n",
    "        \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        for element in self.map_f(*args, **kwargs):\n",
    "            ignore = False\n",
    "            for transform_f in self._transformations:\n",
    "                element = transform_f(element)\n",
    "                if element is None:\n",
    "                    ignore = True\n",
    "                    break\n",
    "            if ignore:\n",
    "                continue\n",
    "            yield element\n",
    "    \n",
    "    def add_filter(self, filter_f):\n",
    "        def wrapper(element):\n",
    "            if filter_f(element):\n",
    "                return element\n",
    "            return None\n",
    "        self.add_transform(wrapper)\n",
    "        return self\n",
    "    \n",
    "    def add_transform(self, transform_f):\n",
    "        self._transformations.append(transform_f)\n",
    "        return self\n",
    "            \n",
    "class LineExtractor(Mapper):\n",
    "    def __init__(self):\n",
    "        def extract_lines(file):\n",
    "            with open(file, \"rt\") as f:\n",
    "                for line in f:\n",
    "                    yield line[:-1]\n",
    "        super(LineExtractor, self).__init__(extract_lines)\n",
    "            \n",
    "    def lower(self):\n",
    "        return self.add_transform(lambda x: x.lower())\n",
    "        \n",
    "            \n",
    "    def bound_length(self, lower_bound=None, upper_bound=None):\n",
    "        if lower_bound:\n",
    "            self.add_filter(lambda x: lower_bound <= len(x))\n",
    "        if lower_bound:\n",
    "            self.add_filter(lambda x: len(x) <= upper_bound)\n",
    "        return self\n",
    "    \n",
    "    def split_spaces(self):\n",
    "        return self.add_transform(lambda x: x.split(' '))\n",
    "\n",
    "def batched_reducer(minibatch_size,\n",
    "                    minibatch_f=lambda x:x,\n",
    "                    examples_until_minibatches=None,\n",
    "                    sorting_key=lambda x: len(x)):\n",
    "    collected = []\n",
    "    examples_until_minibatches = examples_until_minibatches or minibatch_size\n",
    "    def wrapper(el):\n",
    "        collected.append(el)\n",
    "        if len(collected) >= examples_until_minibatches:\n",
    "            collected.sort(key=sorting_key)\n",
    "            for i in range(0, len(collected), minibatch_size):\n",
    "                if i + minibatch_size < len(collected):\n",
    "                    yield minibatch_f(collected[i:(i + minibatch_size)])\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Batch(object):\n",
    "    def __init__(self):\n",
    "        self.timesteps = 0\n",
    "        self.examples  = 0\n",
    "    def inputs(timestep):\n",
    "        return None\n",
    "    def targets(timestep):\n",
    "        return None\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Batch(timesteps=%d, examples=%d)' % (self.timesteps, self.examples)\n",
    "\n",
    "class LMBatch(object):\n",
    "    START_TOKEN = '**START**'\n",
    "    @staticmethod\n",
    "    def given_vocab(vocab, **kwargs):\n",
    "        def wrapper(sentences):\n",
    "            return LMBatch(sentences, vocab, **kwargs)\n",
    "        return wrapper\n",
    "    \n",
    "    def __init__(self, sentences, vocab, store_originals=False, add_eos=True):\n",
    "        if store_originals:\n",
    "            self.sentences = sentences\n",
    "        sentences = [vocab.encode(s, add_eos=add_eos) for s in sentences]\n",
    "\n",
    "        self.sentence_lengths = [len(s) for s in sentences]\n",
    "\n",
    "        self.timesteps = max(self.sentence_lengths)\n",
    "        self.examples  = len(sentences)\n",
    "        # we add one index to account for start of sequence token\n",
    "        self.data = np.empty((self.timesteps + 1, self.examples))\n",
    "        # data is badded by EOS\n",
    "        self.data.fill(vocab.eos)\n",
    "        self.data[0,:].fill(vocab[LMBatch.START_TOKEN])\n",
    "        for example_idx, example in enumerate(sentences):\n",
    "            self.data[1:(len(example) + 1), example_idx] = example\n",
    "        self.data = Mat(self.data, dtype=np.int32)\n",
    "        \n",
    "    def inputs(self, timestep):\n",
    "        return self.data[timestep]\n",
    "    \n",
    "    def targets(self, timestep):\n",
    "         # predictions are offset by 1 to inputs, so\n",
    "        return self.data[timestep + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GLOVE_VOCAB = '/home/sidor/projects/Dali/data/glove/vocab10k.txt'\n",
    "BOOKCORPUS  = '/home/sidor/datasets/bookcorpus/'\n",
    "\n",
    "INPUT_SIZE = 250\n",
    "HIDDENS = [250, 250]\n",
    "MINIBATCH = 512\n",
    "SENTENCES_UNTIL_MINIBATCH = 1000 * MINIBATCH\n",
    "SENTENCE_LENGTH=(2, 10)\n",
    "\n",
    "glove_vocab = Vocab()\n",
    "glove_vocab.add(LineExtractor()(GLOVE_VOCAB))\n",
    "glove_vocab.add(LMBatch.START_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_bookcorpus(path, vocab, minibatch_size, sentences_until_minibatch=None, sentence_length_bounds=(2, 20)):\n",
    "    sentences_until_minibatch = sentences_until_minibatch or 10000 * minibatch_size\n",
    "    files   = discover_files(BOOKCORPUS, \".txt\")\n",
    "    mapper  = LineExtractor() \\\n",
    "              .lower()        \\\n",
    "              .split_spaces() \\\n",
    "              .bound_length(*sentence_length_bounds)\n",
    "    reducer = batched_reducer(minibatch_size,\n",
    "                              LMBatch.given_vocab(glove_vocab, store_originals=True),\n",
    "                              sentences_until_minibatch)\n",
    "    return process_files(files=files, mapper=mapper, reducer=reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Beam = namedtuple(\"Beam\", [\"solution\", \"score\", \"state\"])\n",
    "\n",
    "def beam_search(initial_state,\n",
    "                candidate_scores,\n",
    "                make_choice,\n",
    "                beam_width=5,\n",
    "                eos_symbol = None,\n",
    "                max_sequence_length=None,\n",
    "                forbiden_symbols=[]):\n",
    "    iterations = 0\n",
    "    results = [\n",
    "        Beam([], Mat(1,1), initial_state)\n",
    "    ]\n",
    "    \n",
    "    def lazy_beam(prev_beam, candidate, new_score):\n",
    "        def generate():\n",
    "            return Beam(\n",
    "                prev_beam.solution + [candidate],\n",
    "                new_score,\n",
    "                make_choice(prev_beam.state, candidate),\n",
    "            )           \n",
    "        return generate\n",
    "    \n",
    "    while max_sequence_length is None or iterations < max_sequence_length:\n",
    "        proposals = []\n",
    "        for beam in results:\n",
    "            if (eos_symbol is not None and\n",
    "                    len(beam.solution) > 0 and\n",
    "                    beam.solution[-1] == eos_symbol):\n",
    "                proposals.append((beam.score, lambda: beam))\n",
    "            else:\n",
    "                scores = candidate_scores(beam.state)\n",
    "                sorted_candidates = MatOps.argsort(scores)\n",
    "                sorted_candidates = reversed(sorted_candidates)\n",
    "                candidates_remaining = beam_width\n",
    "                for candidate_idx in sorted_candidates:\n",
    "                    if candidate_idx in forbiden_symbols:\n",
    "                        continue\n",
    "                    new_score = beam.score + scores.T()[candidate_idx]\n",
    "                    proposals.append((new_score, lazy_beam(beam, candidate_idx, new_score)))\n",
    "                    candidates_remaining -= 1\n",
    "                    if candidates_remaining <= 0:\n",
    "                        break\n",
    "        proposals.sort(reverse=True, key=lambda x: x[0].w[0])\n",
    "        results = [ eval_beam() for _, eval_beam in proposals[:beam_width]]\n",
    "    \n",
    "        iterations += 1\n",
    "    return results\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "def beam_search_test():\n",
    "    MAX_LENGTH = 2\n",
    "    choices = {\n",
    "        #initial_choices\n",
    "        \"a\": 0.6,\n",
    "        \"b\": 0.4,\n",
    "        #after chosing a\n",
    "        \"aa\": 0.55,  # (total worth 0.33)\n",
    "        \"ab\": 0.45,  # (total worth 0.18)\n",
    "        #after choosing b\n",
    "        \"ba\": 0.99,  # (total worth 0.495)\n",
    "        \"bb\": 0.11,  # (total worth 0.044)\n",
    "    };\n",
    "\n",
    "    # Above example is designed to demonstrate greedy solution,\n",
    "    # as well as better optimal solution:\n",
    "    # GREEDY:    (beam_width == 1) => \"aa\" worth 0.33\n",
    "    # OPTIMAL:   (beam_width == 2) => \"ba\" worth 0.495\n",
    "    res_aa = Beam([0,0], Mat([math.log(0.6 * 0.55)]), \"aa\")\n",
    "    res_ab = Beam([0,1], Mat([math.log(0.6 * 0.45)]), \"ab\")\n",
    "    res_ba = Beam([1,0], Mat([math.log(0.4 * 0.99)]), \"ba\")\n",
    "    res_bb = Beam([1,1], Mat([math.log(0.4 * 0.11)]), \"bb\")\n",
    "\n",
    "    initial_state = \"\";\n",
    "    def candidate_scores(state):\n",
    "        ret = Mat(1,2)\n",
    "        ret.w[0,0] = math.log(choices[state + \"a\"])\n",
    "        ret.w[0,1] = math.log(choices[state + \"b\"])\n",
    "        return ret\n",
    "    def make_choice(prev_state, choice):\n",
    "        return prev_state + (\"a\" if choice == 0 else \"b\")\n",
    "    \n",
    "    def my_beam_search(beam_width):\n",
    "        return beam_search(initial_state=initial_state,\n",
    "                           candidate_scores=candidate_scores,\n",
    "                           make_choice=make_choice,\n",
    "                           beam_width=beam_width,\n",
    "                           max_sequence_length = MAX_LENGTH)\n",
    "   \n",
    "    def beams_equal(b1, b2):\n",
    "        return (b1.solution == b2.solution and \n",
    "                np.allclose(b1.score.w, b2.score.w) and \n",
    "                b1.state == b2.state)\n",
    "    \n",
    "    def results_equal(a,b):\n",
    "        return len(a) == len(b) and all(beams_equal(b1,b2) for b1,b2 in zip(a,b))\n",
    "\n",
    "    #EXPECT_THROW(my_beam_search(0),std::runtime_error);\n",
    "    assert results_equal(my_beam_search(1), [res_aa])\n",
    "    assert results_equal(my_beam_search(2), [res_ba, res_aa])\n",
    "    assert results_equal(my_beam_search(4), [res_ba, res_aa, res_ab, res_bb])\n",
    "    assert results_equal(my_beam_search(10),[res_ba, res_aa, res_ab, res_bb])\n",
    "    \n",
    "beam_search_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LanguageModel(object):\n",
    "    def __init__(self, input_size, hiddens, vocab_size, dtype=np.float32):\n",
    "        self.input_size = input_size\n",
    "        self.hiddens    = hiddens\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        self.encoder = drandom.uniform(-0.05, 0.05, (vocab_size, input_size), dtype=dtype)\n",
    "        self.lstm = StackedLSTM(input_size, hiddens, dtype=dtype)\n",
    "        self.decoder = Layer(hiddens[-1], vocab_size, dtype=dtype)\n",
    "    \n",
    "    def error(self, batch):\n",
    "        error = Mat(1,1)\n",
    "        state = self.lstm.initial_states()\n",
    "        for ts in range(batch.timesteps):\n",
    "            inputs  = batch.inputs(ts)\n",
    "            targets = batch.targets(ts)\n",
    "            if inputs:\n",
    "                encoded = self.encoder[batch.inputs(ts)]\n",
    "            else:\n",
    "                encoded = Mat(1, self.input_size)\n",
    "            state = self.lstm.activate(encoded, state)\n",
    "            if targets:\n",
    "                decoded = self.decoder.activate(state[-1].hidden)\n",
    "                error = error + MatOps.softmax_cross_entropy(decoded, targets).sum()\n",
    "        return error\n",
    "    \n",
    "    def sample(self, priming, eos_symbol):\n",
    "        with NoBackprop():\n",
    "            state = self.lstm.initial_states()\n",
    "            for word_idx in priming:\n",
    "                encoded = self.encoder[word_idx]\n",
    "                state = self.lstm.activate(encoded, state)\n",
    "            def candidate_scores(state):\n",
    "                return MatOps.softmax(self.decoder.activate(state[-1].hidden)).log()\n",
    "            def make_choice(state, candidate_idx):\n",
    "                encoded = self.encoder[candidate_idx]\n",
    "                return self.lstm.activate(encoded, state)\n",
    "\n",
    "            return beam_search(initial_state    = state,\n",
    "                               candidate_scores = candidate_scores,\n",
    "                               make_choice      = make_choice,\n",
    "                               max_sequence_length = 20)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.encoder] + self.lstm.parameters() + self.decoder.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model  = LanguageModel(INPUT_SIZE, HIDDENS, len(glove_vocab))\n",
    "params = model.parameters()\n",
    "s = AdaDelta(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:              6.19914372762\n",
      "Time per batch:     0.40274477005004883\n",
      "Words per second:   3813.829785571448\n",
      "Batches processed:  1\n",
      "Max batch length:   3\n",
      "0.709602 => [who is] .\n",
      "0.013494 => [who is] \n",
      "0.013212 => [who is] !\n",
      "0.007470 => [who is] **UNK** .\n",
      "0.005285 => [who is] . oh\n"
     ]
    }
   ],
   "source": [
    "total_error, num_words = 0.0, 0\n",
    "\n",
    "reports = Throttled()\n",
    "\n",
    "total_error,    num_words   = 0.0, 0\n",
    "batch_time, num_batches = 0.0, 0\n",
    "\n",
    "def show_reconstructions(words):\n",
    "    for solution, score, _ in model.sample(glove_vocab.encode([LMBatch.START_TOKEN] + words), glove_vocab.eos):\n",
    "        score = math.exp(score.w[0])\n",
    "        priming = ' '.join(words)\n",
    "        solution = ' '.join(glove_vocab.decode(solution, True))\n",
    "        print('%f => [%s] %s' % (score, priming, solution))\n",
    "\n",
    "@throttled(5)\n",
    "def report():\n",
    "    if num_batches == 0 or num_words == 0 or abs(batch_time) < 1e-6:\n",
    "        return\n",
    "    clear_output()\n",
    "    print('Error:             ', total_error / num_words)\n",
    "    print('Time per batch:    ', batch_time  / num_batches)\n",
    "    print('Words per second:  ', num_words   / batch_time )\n",
    "    print('Batches processed: ', num_batches)\n",
    "    print('Max batch length:  ', max_timesteps)\n",
    "    show_reconstructions([\"who\", \"is\"])\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    \n",
    "last_batch = None\n",
    "max_timesteps = 0\n",
    "\n",
    "batch_end_time, batch_start_time = None, None\n",
    "\n",
    "for batch in extract_bookcorpus(BOOKCORPUS, glove_vocab, MINIBATCH, \n",
    "                                SENTENCES_UNTIL_MINIBATCH, sentence_length_bounds=SENTENCE_LENGTH):\n",
    "\n",
    "    last_batch = batch\n",
    "    max_timesteps = max(max_timesteps, batch.timesteps)\n",
    "    \n",
    "    batch_start_time = time.time()\n",
    "    \n",
    "    error = model.error(batch)\n",
    "    error.grad()\n",
    "    Graph.backward()\n",
    "    s.step(params)\n",
    "    batch_end_time = time.time()\n",
    "\n",
    "    total_error += error.w[0, 0]\n",
    "    num_words   += sum(batch.sentence_lengths)\n",
    "    \n",
    "\n",
    "        \n",
    "    if batch_end_time is not None and batch_start_time is not None:\n",
    "        batch_time += batch_end_time - batch_start_time\n",
    "    num_batches    += 1\n",
    "    \n",
    "    report()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7308366f6098>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.empty((3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n",
      "c\n"
     ]
    }
   ],
   "source": [
    "w = []\n",
    "\n",
    "def lazy_beam(c):\n",
    "    def generate():\n",
    "        print(c)\n",
    "    return generate\n",
    "\n",
    "for c in [\"a\", \"b\",\"c\"]:\n",
    "    w.append(lazy_beam(c))\n",
    "\n",
    "for ww in w:\n",
    "    ww()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['**EOS**', 'the', ',']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vocab.decode([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
